import os, sys

from pathlib import Path
path_root = Path(__file__).parents[4]  # upto 'codebase' folder
sys.path.insert(0, str(path_root))
# print(sys.path)

from utils import dl_reproducible_result_util
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import StandardScaler

from utils import PPIPUtils
from postproc.mat_p2ip_pd.pul.preproc_pul import pul_data_prep_4_kFold


# Check if GPU is available and set the device accordingly
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class MLP(nn.Module):
    """
    A simple Multilayer Perceptron (MLP) model for binary classification.

    Attributes:
        fc1 (nn.Linear): First fully connected layer.
        fc2 (nn.Linear): Second fully connected layer.
        fc3 (nn.Linear): Third fully connected layer.
        relu (nn.ReLU): ReLU activation function.
        sigmoid (nn.Sigmoid): Sigmoid activation function for output.
    """
    def __init__(self, input_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        """
        Forward pass through the network.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output probabilities.
        """
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.sigmoid(self.fc3(x))
        return x


def pul_prepare_data(train_train_df=None, train_val_df=None, test_df=None):
    print('inside pul_prepare_data() method - Start')
    # Drop columns 'prot_id', 'chain_1_seq', 'chain_2_seq' from all the dataframes
    col_to_be_dropped = ['prot_id', 'chain_1_seq', 'chain_2_seq']
    train_train_df = train_train_df.drop(col_to_be_dropped, axis=1)
    train_val_df = train_val_df.drop(col_to_be_dropped, axis=1)
    test_df = test_df.drop(col_to_be_dropped, axis=1)

    # label convention: 0 for negative, 1 for positive and -1 for unlabelled
    # Separate labelled and unlabelled data from train_train_df (train_train_df contains 
    # only positive and unlabelled data)
    train_train_positive_data = train_train_df[train_train_df['label'] == 1]
    train_train_positive_data = train_train_positive_data.reset_index(drop=True)
    train_train_unlabelled_data = train_train_df[train_train_df['label'] == -1]
    train_train_unlabelled_data = train_train_unlabelled_data.reset_index(drop=True)

    # Sample from the train_train_unlabelled_data to include in the initial training for finding reliable negatives
    # ## train_train_positive_data_size = train_train_positive_data.shape[0]
    # ## U_sample = train_train_unlabelled_data.sample(n=train_train_positive_data_size, random_state=456)
    U_sample = train_train_unlabelled_data.sample(frac=0.2, random_state=456)
    initial_train_data = pd.concat([train_train_positive_data, U_sample])
    initial_train_data = initial_train_data.reset_index(drop=True)

    # Prepare the training data
    X_train = initial_train_data.drop('label', axis=1).values
    y_train = initial_train_data['label'].replace(-1, 0).values  # Treat sampled unlabelled as negative initially

    X_val = train_val_df.drop('label', axis=1).values
    y_val = train_val_df['label'].values

    X_test = test_df.drop('label', axis=1).values
    y_test = test_df['label'].values

    # Standardize the features
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    X_test = scaler.transform(X_test)

    print('inside pul_prepare_data() method - End')
    return X_train, y_train, X_val, y_val, X_test, y_test, scaler


def calculate_class_weights(y_train):
    """
    Calculate class weights based on the frequency of each class.

    Args:
        y_train (np.array): Training labels.

    Returns:
        torch.Tensor: Class weights for handling imbalance.
    """
    pos_weight = len(y_train) / (2 * np.sum(y_train))
    neg_weight = len(y_train) / (2 * (len(y_train) - np.sum(y_train)))
    class_weights = torch.FloatTensor([neg_weight, pos_weight]).to(device)
    return class_weights


def train_model(X_train, y_train, X_val, y_val, input_size, num_epochs=20, lr=0.001, class_weights=None):
    """
    Train the model and evaluate it on the validation set at each epoch.

    Args:
        X_train (torch.Tensor): Training features.
        y_train (torch.Tensor): Training labels.
        X_val (torch.Tensor): Validation features.
        y_val (torch.Tensor): Validation labels.
        input_size (int): Number of input features.
        num_epochs (int, optional): Number of epochs for training. Defaults to 20.
        lr (float, optional): Learning rate. Defaults to 0.001.
        class_weights (torch.Tensor, optional): Class weights for handling imbalance. Defaults to None.

    Returns:
        Trained model.
    """
    # Convert to PyTorch tensors
    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)
    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)
    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)
    y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)

    # Initialize the model, loss function, and optimizer
    model = MLP(input_size).to(device)
    # criterion = nn.BCELoss()
    criterion = nn.BCELoss(weight=class_weights) if class_weights is not None else nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # Train the model
    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

        # Validation
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs.squeeze(), y_val)
        
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')
    
    return model


def identify_reliable_negatives(model, unlabelled_data, scaler, threshold=0.1):
    """
    Identify reliable negatives from the unlabelled data.

    Args:
        model: Trained model.
        unlabelled_data (pd.DataFrame): Unlabelled data.
        scaler (StandardScaler): Scaler used for data normalization.
        threshold (float, optional): Threshold to identify reliable negatives. Defaults to 0.1.

    Returns:
        pd.DataFrame: Reliable negatives identified from the unlabelled data.
    """
    X_unlabelled = scaler.transform(unlabelled_data.drop('label', axis=1).values)
    X_unlabelled = torch.tensor(X_unlabelled, dtype=torch.float32).to(device)

    model.eval()
    with torch.no_grad():
        unlabelled_pred_proba = model(X_unlabelled).cpu().numpy()

    reliable_negatives = unlabelled_data[unlabelled_pred_proba.flatten() < threshold]
    return reliable_negatives


def retrain_model(model, new_train_data, X_val, y_val, scaler, num_epochs=20, class_weights=None):
    """
    Retrain the model on the new training data with validation.

    Args:
        model: Initial trained  model.
        new_train_data (pd.DataFrame): New training data including reliable negatives.
        X_val (np.array): Validation features.
        y_val (np.array): Validation labels.
        scaler (StandardScaler): Scaler used for data normalization.
        num_epochs (int, optional): Number of epochs for retraining. Defaults to 20.
        class_weights (torch.Tensor, optional): Class weights for handling imbalance. Defaults to None.

    Returns:
        Retrained model.
    """
    X_new_train = new_train_data.drop('label', axis=1).values
    y_new_train = new_train_data['label'].values

    # Standardize the new training features
    X_new_train = scaler.transform(X_new_train)
    X_new_train = torch.tensor(X_new_train, dtype=torch.float32).to(device)
    y_new_train = torch.tensor(y_new_train, dtype=torch.float32).unsqueeze(1).to(device)

    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)
    y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)

    criterion = nn.BCELoss(weight=class_weights) if class_weights is not None else nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Retrain the model
    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()

        outputs = model(X_new_train)
        loss = criterion(outputs.squeeze(), y_new_train)
        loss.backward()
        optimizer.step()

        # Validation
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs.squeeze(), y_val)
        
        print(f'Retrain Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')
    
    return model


def evaluate_model(model, X_test, y_test):
    """
    Evaluate the model on labelled test data

    Args:
        model: Trained model to be evaluated.
        X_test: Test data feature set.
        y_test: Test data labels.

    Returns:
        dictionary: Various performance scores of the model on the labelled data.
    """
    model.eval()
    
    # Prepare the test data
    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)
    y_test = torch.tensor(y_test, dtype=torch.float32).to(device)

    # Perform inference
    with torch.no_grad():
        y_pred_proba = model(X_test)

    # Assuming y_test is the ground truth labels and y_pred_proba contains predicted probabilities
    # Calculate metrics
    y_pred_proba = y_pred_proba.cpu().numpy()
    y_test = y_test.cpu().numpy()

    y_pred = (y_pred_proba >= 0.5).astype(int)  # Convert probabilities to binary predictions
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc_score = roc_auc_score(y_test, y_pred_proba)

    # Store metrics in a dictionary
    eval_score_dict = {
        'accuracy': round(accuracy, 3),
        'precision': round(precision, 3),
        'recall': round(recall, 3),
        'F1_score': round(f1, 3),
        'auc_score': round(auc_score, 3)
    }

    # Return the dictionary
    return eval_score_dict


def save_model(model=None, root_path='./', itr_tag=None, fold_index=0):
    """
    Save the trained model to the specified path.
    """
    model_path = os.path.join(root_path, 'dataset/postproc_data/pul_result', itr_tag, 'proc_pul')
    model_name = f'pul_model_fold_{fold_index}.pth'
    torch.save(model.state_dict(), os.path.join(model_path, model_name))
    print(f'model is saved at {os.path.join(model_path, model_name)}')


def perform_pu_learning(root_path='./', itr_tag=None, fold_index=0, train_train_df=None, train_val_df=None, test_df=None, hparams={}):
    """
    Main method to execute PU learning.
    """
    print(f'\n ########## fold_index:{fold_index} :: inside perform_pu_learning() method - Start')
    # prepare the data
    print(f'\n ########## fold_index:{fold_index} :: Preparing the data for the PU learning...\n')
    X_train, y_train, X_val, y_val, X_test, y_test, scaler = pul_prepare_data(train_train_df=train_train_df, train_val_df=train_val_df, test_df=test_df)
    
    # Train the model on the initial dataset
    print(f'\n ########## fold_index:{fold_index} :: Training the model on the initial dataset...\n')
    # Calculate class weights for y_train
    class_weights_y_train = calculate_class_weights(y_train)
    input_size = X_train.shape[1]
    model = train_model(X_train, y_train, X_val, y_val, input_size, num_epochs=hparams['num_epochs'], lr=hparams['lr'], class_weights=class_weights_y_train)

    # Identify reliable negatives
    print(f'\n ########## fold_index:{fold_index} :: Identifying reliable negatives...\n')
    # label convention: 0 for negative, 1 for positive and -1 for unlabelled
    # train_train_df contains only positive and unlabelled data
    train_train_df = train_train_df.drop(['prot_id', 'chain_1_seq', 'chain_2_seq'], axis=1)
    train_train_unlabelled_data = train_train_df[train_train_df['label'] == -1].reset_index(drop=True)
    reliable_negatives = identify_reliable_negatives(model, train_train_unlabelled_data, scaler, threshold=hparams['threshold'])

    # Create new training data using reliable_negatives
    print(f'\n ########## fold_index:{fold_index} :: Creating new training data using reliable_negatives...\n')
    train_train_positive_data = train_train_df[train_train_df['label'] == 1].reset_index(drop=True)
    new_train_data = pd.concat([train_train_positive_data, reliable_negatives])
    # Treat reliable-negative data-points as negative.
    new_train_data['label'] = new_train_data['label'].replace(-1, 0)
    # Calculate class weights
    class_weights = calculate_class_weights(new_train_data['label'].values)
    
    # Retrain the model on the new training dataset
    print(f'\n ########## fold_index:{fold_index} :: Retraining the model on the new training dataset...\n')
    model = retrain_model(model, new_train_data, X_val, y_val, scaler, num_epochs=hparams['num_epochs'], class_weights=class_weights)

    # Save the model
    print(f'\n ########## fold_index:{fold_index} :: Saving the model...\n')
    save_model(model=model, root_path=root_path, itr_tag=itr_tag, fold_index=fold_index)

    # Evaluate the model on the test set
    print(f'\n ########## fold_index:{fold_index} :: Evaluating the model on the test set...\n')
    eval_score_dict = evaluate_model(model, X_test, y_test)
    print(f'\n ########## fold_index:{fold_index} :: eval_score_dict: \n {eval_score_dict}\n')

    print(f'\n ########## fold_index:{fold_index} :: inside perform_pu_learning() method - End')
    return eval_score_dict



if __name__ == '__main__':
    root_path = os.path.join('/project/root/directory/path/here')
    root_path = os.path.join('/scratch/pralaycs/Shubh_Working_Remote/PPI_Wkspc/PPI_Code/matpip_pd_prj')

    itr_tag = 'mcmc_fullLen_puFalse_batch5_mutPrcntLen10'
    fold_index = 0
    
    # #################### Hyper-parameters -Start
    hparams = {}
    hparams['num_epochs'] = 2  # Number of epochs for training and retraining
    hparams['lr'] = 0.001  # Learning rate for the optimizer
    hparams['threshold'] = 0.1  # Threshold to identify reliable negatives
    # #################### Hyper-parameters -End

    train_train_df, train_val_df, test_df = pul_data_prep_4_kFold.fetch_fold_data(root_path=root_path, itr_tag=itr_tag, fold_index=fold_index)
    fold_result_dict = perform_pu_learning(root_path=root_path, itr_tag=itr_tag, fold_index=fold_index
                                            , train_train_df=train_train_df, train_val_df=train_val_df, test_df=test_df
                                            , hparams=hparams)
    print(f'\n fold_result_dict: \n {fold_result_dict}')
